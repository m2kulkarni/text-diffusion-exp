{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join('input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with llama 3 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "train_ids = tokenizer.encode(train_data, add_special_tokens=False)\n",
    "val_ids   = tokenizer.encode(val_data,   add_special_tokens=False)\n",
    "\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join('train.bin'))\n",
    "val_ids.tofile(os.path.join('val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1843676800.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[95], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from data import ShakespeareDiffusionDataset= len(tokenizer)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from data import ShakespeareDiffusionDataset= len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m B, T \u001b[38;5;241m=\u001b[39m X_s_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m----> 3\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m \u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_s_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_t\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():                          \u001b[38;5;66;03m# no grads for targets\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     emb_s \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39mwte(X_s_ids)                 \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/text-diffusion-exp/model.py:159\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, time_s, time_t)\u001b[0m\n\u001b[1;32m    157\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmup_input_alpha \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmup_enabled \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 159\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m v_theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)  \u001b[38;5;66;03m# (B,T,D)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_theta\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/text-diffusion-exp/model.py:83\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 83\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_scale \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_scale \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/text-diffusion-exp/model.py:49\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 49\u001b[0m     B,T,C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     50\u001b[0m     q,k,v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(B,T,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (B,h,T,hd)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "B, T = X_s_ids.size()\n",
    "\n",
    "v_pred = flow(X_s_ids, time_s, time_t)        # (B, T, D)\n",
    "\n",
    "with torch.no_grad():                          # no grads for targets\n",
    "    emb_s = flow.wte(X_s_ids)                 # (B, T, D)\n",
    "    emb_t = flow.wte(X_t_ids)                 # (B, T, D)\n",
    "\n",
    "scale   = (time_s - time_t).view(B, 1, 1)      # broadcast to (B,1,1)\n",
    "v_true  = scale * (emb_s - emb_t)              # (B, T, D)\n",
    "\n",
    "loss = F.mse_loss(v_pred, v_true)              # scalar\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "* Architecture (Transformer Encoder)\n",
    "* Data Generation (Shakespeare text)\n",
    "* Loss function\n",
    "* Masking\n",
    "* Training loop\n",
    "* Checkpointing\n",
    "* Llama-3 tokenizer \n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Engineering Suggestions for a Flow-Matching Transformer\n",
    "Here's a breakdown of suggestions for designing your Transformer-based neural network for text generation using flow-matching with a Llama-3 tokenizer.\n",
    "\n",
    "1. Input Processing:\n",
    "Token Embeddings:\n",
    "Use an nn.Embedding layer from PyTorch to convert your input token IDs (from the Llama-3 tokenizer) into dense vector representations.\n",
    "The size of the vocabulary will be determined by your Llama-3 tokenizer.\n",
    "The embedding dimension (d_model) is a key hyperparameter. Typical values range from 256 to 1024 or higher, depending on model capacity and computational resources.\n",
    "Time Embedding:\n",
    "The scalar time input t (presumably normalized, e.g., t âˆˆ [0, 1]) needs to be converted into a vector representation. Common techniques include:\n",
    "Sinusoidal Positional Encoding: Similar to how positional information is often encoded in Transformers. You can create a fixed sinusoidal embedding based on the time value. This helps the model understand the relative \"position\" in the flow.\n",
    "\n",
    "Example:\n",
    " d_time_emb = d_model // 4 (or some other dimension)\n",
    "time_embedding = torch.zeros(1, d_time_emb)\n",
    "div_term = torch.exp(torch.arange(0, d_time_emb, 2).float() * -(math.log(10000.0) / d_time_emb))\n",
    "time_embedding[0, 0::2] = torch.sin(time_input * div_term)\n",
    "time_embedding[0, 1::2] = torch.cos(time_input * div_term)\n",
    "\n",
    "Learned Linear Layers (MLP): Pass the time t through a small multi-layer perceptron (e.g., two linear layers with a non-linearity like SiLU or GELU) to produce a time embedding vector. This offers more flexibility.\n",
    "Gaussian Fourier Features: Project t using random Gaussian features and then apply sinusoidal functions. This is common in score-based models and can be effective here.\n",
    "The dimension of this time embedding should ideally be compatible with d_model for easy integration.\n",
    "Combining Token and Time Embeddings:\n",
    "Initial Combination:\n",
    "Addition: The simplest way is to broadcast the time embedding and add it to the token embeddings. If your token embeddings are (L, d_model) and your time embedding is (d_model), you can add them directly.\n",
    "Concatenation + Projection: Concatenate the time embedding to each token embedding along the feature dimension and then use a linear layer to project it back to d_model. This is more expressive but adds parameters.\n",
    "Positional Encoding for Tokens: Don't forget to add standard positional encodings (sinusoidal or learned) to your token embeddings to give the model information about the sequence order. This is separate from the time embedding. The order would typically be: token_ids -> token_embedding -> + positional_encoding -> + time_embedding_broadcasted.\n",
    "2. Transformer Architecture (Encoder-Style):\n",
    "\n",
    "Since your input and output are sequences of the same length L and you're essentially learning a vector field v(x_t, t), an encoder-style Transformer (like BERT's architecture but without the pre-training tasks) is suitable.\n",
    "Transformer Blocks:\n",
    "Consist of a Multi-Head Self-Attention (MHSA) layer followed by a position-wise Feed-Forward Network (FFN).\n",
    "Employ residual connections around each sub-layer.\n",
    "Use Layer Normalization (nn.LayerNorm) after each sub-layer (i.e., post-LN) or before each sub-layer and after the residual addition (pre-LN, often more stable).\n",
    "Multi-Head Self-Attention (MHSA):\n",
    "Standard implementation. d_model should be divisible by the number of heads (num_heads).\n",
    "No masking is typically needed beyond padding masks if your sequences have variable lengths (though you specified a fixed length L). For flow matching, you generally want each token to attend to all other tokens to model the joint probability.\n",
    "Feed-Forward Network (FFN):\n",
    "Usually two linear layers with a non-linearity in between (e.g., GELU, SiLU, ReLU).\n",
    "The inner dimension is often 4 * d_model.\n",
    "Number of Layers: This (num_layers) is another key hyperparameter determining model depth and capacity.\n",
    "3. Time Conditioning within the Transformer (Advanced):\n",
    "\n",
    "While adding time embedding at the input is a good start, more sophisticated conditioning can improve performance.\n",
    "Adaptive Layer Normalization (AdaLN) / FiLM Layers:\n",
    "Instead of just adding the time embedding at the beginning, use it to modulate the activations within each Transformer block.\n",
    "The time embedding can be projected to produce scale (gamma) and shift (beta) parameters for Layer Normalization layers.\n",
    "output = gamma * LayerNorm(input) + beta\n",
    "This allows the time t to influence the processing at different depths of the network more directly.\n",
    "Cross-Attention with Time Embedding:\n",
    "Treat the time embedding as a separate conditioning signal and use cross-attention mechanisms within the Transformer blocks, where token embeddings query the time embedding. This is less common for this specific setup but a possibility.\n",
    "4. Output Processing:\n",
    "The final output of your Transformer stack will be a tensor of shape (L, d_model).\n",
    "You require the output to be \"another tensor of the same shape: embedding dimension by L tokens\". This implies your final output should directly represent the vector field or flow direction in the embedding space.\n",
    "So, the output of the last Transformer block can be directly used as the output of your neural network v(x_t, t). No additional projection to vocabulary space (like in typical language models for generation) is needed at this stage, as you're predicting the flow, not the next token probabilities.\n",
    "5. Llama-3 Tokenizer Specifics:\n",
    "Ensure you correctly handle padding tokens if you batch sequences of varying lengths. The attention mechanism should ignore padding tokens.\n",
    "The vocabulary size for your initial nn.Embedding layer is determined by tokenizer.vocab_size.\n",
    "The embedding vectors learned by Llama-3 (if you were using its pre-trained embeddings) are high-dimensional. You can choose to initialize your nn.Embedding layer randomly or try to use a projection of Llama-3's embeddings if that makes sense for your flow-matching setup (though often for these conditional models, embeddings are learned from scratch or fine-tuned extensively).\n",
    "6. PyTorch Implementation Details:\n",
    "Modularity: Define nn.Module classes for:\n",
    "Time Embedding (e.g., SinusoidalTimeEmbedding, MLPTimeEmbedding)\n",
    "Transformer Block (containing MHSA, FFN, LayerNorm, residuals)\n",
    "The full Transformer model (stacking the blocks and handling input/output embeddings).\n",
    "Device Management: Use .to(device) for tensors and models.\n",
    "Gradient Clipping: Can be useful for stabilizing training.\n",
    "Dropout: Apply dropout within Transformer blocks (e.g., after attention, in FFNs) for regularization.\n",
    "7. Key Hyperparameters to Tune:\n",
    "d_model (embedding dimension)\n",
    "num_layers (number of Transformer blocks)\n",
    "num_heads (number of attention heads)\n",
    "d_ffn (inner dimension of FFNs)\n",
    "d_time_emb (dimension of the raw time embedding before potential projection)\n",
    "Learning rate\n",
    "Batch size\n",
    "Dropout rates\n",
    "Diagrammatic Flow:\n",
    "Input Tokens (L) -> nn.Embedding -> (L, d_model) Token Embeddings\n",
    "Add Positional Encodings -> (L, d_model)\n",
    "Time Input (scalar t) -> Time Embedding Layer -> (d_time_emb) or (d_model) Time Embedding Vector\n",
    "Combine Token Embeddings and Time Embedding (e.g., add broadcasted time embedding) -> (L, d_model) Conditioned Input\n",
    "Pass through N Transformer Blocks (each potentially using AdaLN with the time embedding)\n",
    "Output: (L, d_model) Tensor representing the flow/vector field.\n",
    "This detailed outline should provide a strong foundation for building your flow-matching Transformer. Remember to start with a simpler configuration and gradually increase complexity as you debug and iterate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
